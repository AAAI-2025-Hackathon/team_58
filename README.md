# EcoLLM: Sustainable AI Routing ğŸŒâš¡  
### Optimizing AI Model Selection for Cost, Performance, and Sustainability  

## ğŸš€ Project Overview  
**EcoLLM** is a carbon-aware LLM router that dynamically selects the most optimal AI model for a given query, balancing **accuracy, inference time, cost, and environmental impact**. By leveraging a **Pareto-optimal approach**, EcoLLM ensures that queries are routed to models that **minimize COâ‚‚ emissions while maximizing efficiency**.

---

## ğŸ¯ Problem Statement  
Large Language Models (LLMs) **consume significant computational resources**, leading to **high energy usage, cost, and carbon footprint**. However, **not all queries require the most powerful models**â€”some can be handled by **more energy-efficient options** without sacrificing quality.

---

## ğŸŒ± Our Solution  
EcoLLM **intelligently routes queries** to the most suitable model using:  

âœ… **Query Analysis**: Categorizes queries (**medical, finance, general, etc.**) and assesses **required accuracy, latency, and cost sensitivity**.  
âœ… **Pareto Optimization**: Selects the **best model** based on a **multi-objective tradeoff** between cost, energy efficiency, and performance.  
âœ… **Dynamic Model Selection**: Supports **GPT-4o, Claude-3, and Grok-2**, assigning each query to the best-suited LLM.  
âœ… **Carbon Impact Metrics**: Calculates **COâ‚‚ savings** and **real-world environmental benefits** for each query.  

---

## ğŸ” Key Features  
ğŸ“Œ **Smart LLM Selection**: Queries are matched to the most **cost-effective and eco-friendly model**.  
ğŸ“Œ **Real-Time COâ‚‚ Tracking**: Displays the **estimated energy consumption and carbon offset** for each query.  
ğŸ“Œ **Interactive Visualization**: A **3D Pareto tradeoff matrix** highlights the **optimal model selection**.  
ğŸ“Œ **Eco-Friendly AI Metrics**: Translates **COâ‚‚ savings** into real-world equivalents like **bike rides replaced** and **plastic bottles saved**.  

---

## ğŸ’¡ How It Works  
1ï¸âƒ£ **User submits a query**.  
2ï¸âƒ£ **EcoLLM analyzes** the query type (e.g., **medical, math, finance, technology**).  
3ï¸âƒ£ **The best LLM is selected** based on accuracy, latency, and energy efficiency.  
4ï¸âƒ£ **The model generates a response** while tracking cost, inference time, and COâ‚‚ emissions.  
5ï¸âƒ£ **Users see environmental impact metrics** for their query.  

---

## ğŸ“Š Technologies Used  
ğŸ”¹ **Python & Flask** â€“ Backend API for **query processing and model routing**.  
ğŸ”¹ **OpenAI, Anthropic, xAI APIs** â€“ LLM inference calls to **GPT-4o, Claude-3.5 Sonnet, and Grok-2**.  
ğŸ”¹ **Streamlit & Plotly** â€“ **Interactive UI** with **3D Pareto visualizations**.  
ğŸ”¹ **Sustainability Metrics** â€“ COâ‚‚ emission calculations using **Hugging Face Leaderboard methodology**.  

---

## ğŸ–¥ï¸ Demo & Walkthrough  
ğŸ“½ï¸ **Demo Video Link**: [(https://www.youtube.com/watch?v=bQcoEPr3MMU)]

---

## ğŸ’¾ Setup Instructions  

### 1ï¸âƒ£ Clone the repository:  
```bash
git clone https://github.com/your-repo/ecollm.git
cd ecollm



# Check-In

- Title of your submission: **[LLM AutoPilot]**
- Team Members: [Anupama Garani](mailto:anupamagarani95@gmail.com)
- [x] All team members agree to abide by the [Hackathon Rules](https://aaai.org/conference/aaai/aaai-25/hackathon/)
- [x] This AAAI 2025 hackathon entry was created by the team during the period of the hackathon, February 17 â€“ February 24, 2025
- [x] The entry includes a 2-minute maximum length demo video here: [Link](https://www.youtube.com/watch?v=bQcoEPr3MMU)
- [x] The entry clearly identifies the selected theme in the README and the video.
