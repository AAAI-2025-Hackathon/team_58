# EcoLLM: Sustainable AI Routing 🌍⚡  
### Optimizing AI Model Selection for Cost, Performance, and Sustainability  

## 🚀 Project Overview  
**EcoLLM** is a carbon-aware LLM router that dynamically selects the most optimal AI model for a given query, balancing **accuracy, inference time, cost, and environmental impact**. By leveraging a **Pareto-optimal approach**, EcoLLM ensures that queries are routed to models that **minimize CO₂ emissions while maximizing efficiency**.

---

## 🎯 Problem Statement  
Large Language Models (LLMs) **consume significant computational resources**, leading to **high energy usage, cost, and carbon footprint**. However, **not all queries require the most powerful models**—some can be handled by **more energy-efficient options** without sacrificing quality.

---

## 🌱 Our Solution  
EcoLLM **intelligently routes queries** to the most suitable model using:  

✅ **Query Analysis**: Categorizes queries (**medical, finance, general, etc.**) and assesses **required accuracy, latency, and cost sensitivity**.  
✅ **Pareto Optimization**: Selects the **best model** based on a **multi-objective tradeoff** between cost, energy efficiency, and performance.  
✅ **Dynamic Model Selection**: Supports **GPT-4o, Claude-3, and Grok-2**, assigning each query to the best-suited LLM.  
✅ **Carbon Impact Metrics**: Calculates **CO₂ savings** and **real-world environmental benefits** for each query.  

---

## 🔍 Key Features  
📌 **Smart LLM Selection**: Queries are matched to the most **cost-effective and eco-friendly model**.  
📌 **Real-Time CO₂ Tracking**: Displays the **estimated energy consumption and carbon offset** for each query.  
📌 **Interactive Visualization**: A **3D Pareto tradeoff matrix** highlights the **optimal model selection**.  
📌 **Eco-Friendly AI Metrics**: Translates **CO₂ savings** into real-world equivalents like **bike rides replaced** and **plastic bottles saved**.  

---

## 💡 How It Works  
1️⃣ **User submits a query**.  
2️⃣ **EcoLLM analyzes** the query type (e.g., **medical, math, finance, technology**).  
3️⃣ **The best LLM is selected** based on accuracy, latency, and energy efficiency.  
4️⃣ **The model generates a response** while tracking cost, inference time, and CO₂ emissions.  
5️⃣ **Users see environmental impact metrics** for their query.  

---

## 📊 Technologies Used  
🔹 **Python & Flask** – Backend API for **query processing and model routing**.  
🔹 **OpenAI, Anthropic, xAI APIs** – LLM inference calls to **GPT-4o, Claude-3.5 Sonnet, and Grok-2**.  
🔹 **Streamlit & Plotly** – **Interactive UI** with **3D Pareto visualizations**.  
🔹 **Sustainability Metrics** – CO₂ emission calculations using **Hugging Face Leaderboard methodology**.  

---

## 🖥️ Demo & Walkthrough  
📽️ **Demo Video Link**: [(https://www.youtube.com/watch?v=bQcoEPr3MMU)]

---

## 💾 Setup Instructions  

### 1️⃣ Clone the repository:  
```bash
git clone https://github.com/your-repo/ecollm.git
cd ecollm



# Check-In

- Title of your submission: **[LLM AutoPilot]**
- Team Members: [Anupama Garani](mailto:anupamagarani95@gmail.com)
- [x] All team members agree to abide by the [Hackathon Rules](https://aaai.org/conference/aaai/aaai-25/hackathon/)
- [x] This AAAI 2025 hackathon entry was created by the team during the period of the hackathon, February 17 – February 24, 2025
- [x] The entry includes a 2-minute maximum length demo video here: [Link](https://www.youtube.com/watch?v=bQcoEPr3MMU)
- [x] The entry clearly identifies the selected theme in the README and the video.
